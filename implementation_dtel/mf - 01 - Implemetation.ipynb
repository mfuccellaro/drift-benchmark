{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "\n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[index] < value:\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "\t# count all samples at split point\n",
    "\tn_instances = float(sum([len(group) for group in groups]))\n",
    "\t# sum weighted Gini index for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in groups:\n",
    "\t\tsize = float(len(group))\n",
    "\t\t# avoid divide by zero\n",
    "\t\tif size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tscore = 0.0\n",
    "\t\t# score the group based on the score for each class\n",
    "\t\tfor class_val in classes:\n",
    "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
    "\t\t\tscore += p * p\n",
    "\t\t# weight the group score by its relative size\n",
    "\t\tgini += (1.0 - score) * (size / n_instances)\n",
    "\treturn gini\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\tfor index in range(len(dataset[0])-1):\n",
    "\t\tfor row in dataset:\n",
    "\t\t\tgroups = test_split(index, row[index], dataset)\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "\toutcomes = [row[-1] for row in group]\n",
    "\treturn max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
    "\t\treturn\n",
    "\t# check for max depth\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "\t\treturn\n",
    "\t# process left child\n",
    "\tif len(left) <= min_size:\n",
    "\t\tnode['left'] = to_terminal(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = get_split(left)\n",
    "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
    "\t# process right child\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = to_terminal(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = get_split(right)\n",
    "\t\tsplit(node['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "\troot = get_split(train)\n",
    "\tsplit(root, max_depth, min_size, 1)\n",
    "\treturn root\n",
    "\n",
    "# Print a decision tree\n",
    "def print_tree(node, depth=0):\n",
    "\tif isinstance(node, dict):\n",
    "\t\tprint('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
    "\t\tprint_tree(node['left'], depth+1)\n",
    "\t\tprint_tree(node['right'], depth+1)\n",
    "\telse:\n",
    "\t\tprint('%s[%s]' % ((depth*' ', node)))\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict_(node, row):\n",
    "\tif row[int(node['index'])] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict):\n",
    "\t\t\treturn predict_(node['left'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn predict_(node['right'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']\n",
    "\n",
    "def predict(tree, df):\n",
    "    pred = []\n",
    "    try:\n",
    "        df = df.to_numpy()\n",
    "    except:\n",
    "        pass\n",
    "    for row in df:\n",
    "        pred.append(predict_(tree, row))\n",
    "    return pred\n",
    "\n",
    "def map_row_to_tree(node, row, tree, path=[]):\n",
    "    '''for a given row returns the path to find leaf'''\n",
    "    if row[node['index']] < node['value']:\n",
    "        #print(row[node['index']], node['value'])\n",
    "        path.append('left')\n",
    "        if isinstance(node['left'], dict):\n",
    "            return map_row_to_tree(node['left'], row, tree, path)\n",
    "        else:\n",
    "            return path\n",
    "    else:\n",
    "        #print(row[node['index']], node['value'])\n",
    "        path.append('right')\n",
    "        if isinstance(node['right'], dict):\n",
    "            return map_row_to_tree(node['right'], row, tree, path)\n",
    "        else:\n",
    "            return path\n",
    "\n",
    "def access_and_modify(tree, new_res):\n",
    "    '''Modify weights of a tree with new data mapped to leaf.'''\n",
    "    for i in tree.keys():\n",
    "        if i == 'left' or i == 'right':\n",
    "            if not isinstance(tree[i], dict):\n",
    "                if len(new_res[(new_res[0] == i)]) > 0:\n",
    "                    tree[i] = float(new_res[(new_res[0] == i)]['target'])\n",
    "            else:\n",
    "                for j in tree[i].keys():\n",
    "                    if j == 'left' or j == 'right':\n",
    "                        if not isinstance(tree[i][j], dict):\n",
    "                            if len(new_res[(new_res[0] == i) & (new_res[1] == j)]) > 0:\n",
    "                                tree[i][j] = float(new_res[(new_res[0] == i) & (new_res[1] == j)]['target'])\n",
    "                        else:\n",
    "                            for k in tree[i][j].keys():\n",
    "                                if k == 'left' or k == 'right':\n",
    "                                    if not isinstance(tree[i][j][k], dict):\n",
    "                                        try:\n",
    "                                            if len(new_res[(new_res[0] == i) & (new_res[1] == j) & (new_res[2] == k)]) > 0:\n",
    "                                                tree[i][j][k] = float(new_res[(new_res[0] == i) & (new_res[1] == j) & (new_res[2] == k)]['target'])\n",
    "                                        except:\n",
    "                                            pass\n",
    "                                    else:\n",
    "                                        for l in tree[i][j][k].keys():\n",
    "                                            if l == 'left' or l == 'right':\n",
    "                                                if not isinstance(tree[i][j][k][l], dict):\n",
    "                                                    try:\n",
    "                                                        if len(new_res[(new_res[0] == i) & (new_res[1] == j) & (new_res[2] == k) & (new_res[3] == l)]) > 0:\n",
    "                                                            tree[i][j][k][l] = float(new_res[(new_res[0] == i) & (new_res[1] == j) & (new_res[2] == k) & (new_res[3] == l)]['target'])\n",
    "                                                    except:\n",
    "                                                        pass\n",
    "    return tree\n",
    "\n",
    "def modify_tree(tree, df, new_target, max_depth, min_size):\n",
    "    '''Modify leaf with new data by going deeper'''\n",
    "    for i in tree.keys():\n",
    "        if i == 'left' or i == 'right':\n",
    "            if not isinstance(tree[i], dict):\n",
    "                depth = 1\n",
    "                grouped_data = df[df.index.isin(new_target[(new_target[0] == i)].index)]\n",
    "                grouped_data = grouped_data.to_numpy()\n",
    "                if len(grouped_data) > min_size:\n",
    "                    tree[i] = build_tree(grouped_data, max_depth-depth, min_size)\n",
    "                \n",
    "            else:\n",
    "                for j in tree[i].keys():\n",
    "                    if j == 'left' or j == 'right':\n",
    "                        if not isinstance(tree[i][j], dict):\n",
    "                            depth = 2\n",
    "                            grouped_data = df[df.index.isin(new_target[(new_target[0] == i) & (new_target[1] == j)].index)]\n",
    "                            grouped_data = grouped_data.to_numpy()\n",
    "                            if len(grouped_data) > min_size:\n",
    "                                tree[i][j] = build_tree(grouped_data, max_depth-depth, min_size)\n",
    "                        else:\n",
    "                            for k in tree[i][j].keys():\n",
    "                                if k == 'left' or k == 'right':\n",
    "                                    if not isinstance(tree[i][j][k], dict):\n",
    "                                        try:\n",
    "                                            depth = 3\n",
    "                                            grouped_data = df[df.index.isin(new_target[(new_target[0] == i) & (new_target[1] == j)  & \n",
    "                                                                         (new_target[2] == k)].index)]\n",
    "                                            grouped_data = grouped_data.to_numpy()\n",
    "                                            if len(grouped_data) > min_size:\n",
    "                                                tree[i][j][k] = build_tree(grouped_data, max_depth-depth, min_size)\n",
    "                                        except:\n",
    "                                            pass\n",
    "                                        \n",
    "                                    else:\n",
    "                                        for l in tree[i][j][k].keys():\n",
    "                                            if l == 'left' or l == 'right':\n",
    "                                                #print(i,j,k,l)\n",
    "                                                if not isinstance(tree[i][j][k][l], dict):\n",
    "                                                    try:\n",
    "                                                        depth = 4\n",
    "                                                        grouped_data = df[df.index.isin(new_target[(new_target[0] == i) & (new_target[1] == j)  &\n",
    "                                                                                     (new_target[2] == k) & (new_target[3] == l)].index)]\n",
    "                                                        grouped_data = grouped_data.to_numpy()\n",
    "                                                        if len(grouped_data) > min_size:\n",
    "                                                            tree[i][j][k][l] = build_tree(grouped_data, max_depth-depth, min_size)\n",
    "                                                    except:\n",
    "                                                        pass\n",
    "                                                    \n",
    "    return tree\n",
    "\n",
    "def adapt_tree(tree, df, max_depth, min_sample_size):\n",
    "    '''With new data modify values of leaf of tree and go deeper'''\n",
    "    groups = []\n",
    "    new_tree = tree.copy()\n",
    "    for row in df.to_numpy():\n",
    "        path = map_row_to_tree(new_tree, row, new_tree, [])\n",
    "        groups.append(path)\n",
    "    new_target= pd.DataFrame(groups)\n",
    "    new_target['target'] = list(df[list(df.columns)[-1]])\n",
    "    new_target = new_target.fillna('NO')\n",
    "    for i in range(max_depth, 0, -1):\n",
    "        try:\n",
    "            new_target_group = new_target.groupby([j for j in range(i)]).mean().reset_index()\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "    # print(new_target_group)\n",
    "    new_tree = access_and_modify(new_tree, new_target_group)\n",
    "    #print_tree(new_tree)\n",
    "    #print(new_target_group)\n",
    "    new_tree_2 = modify_tree(new_tree, df, new_target, max_depth=max_depth, min_size=min_sample_size)\n",
    "    return new_tree_2\n",
    "\n",
    "def mse_i_t(model_pool, df):\n",
    "    w = []\n",
    "    for i in range(len(model_pool)-1):\n",
    "        #print(i)\n",
    "        m = model_pool[i]\n",
    "        w.append(compute_weights(m, df))\n",
    "    # last model is of weight highest\n",
    "    w.append(1)\n",
    "    return w\n",
    "\n",
    "def compute_weights(m1, df):\n",
    "    eps = 1\n",
    "    try:\n",
    "        df = df.to_numpy()\n",
    "    except:\n",
    "        pass\n",
    "    y = list(row[-1] for row in df)\n",
    "    p = pd.Series(predict(m1, df))\n",
    "    return 1/(sum((p-y)**2)/len(df)+eps)\n",
    "\n",
    "\n",
    "def compute_divS(model_pool_minus_one, df, y):\n",
    "    sum_q = 0\n",
    "    for i in range(len(model_pool_minus_one)):\n",
    "        for j in range(i-1):\n",
    "            m1 = model_pool_minus_one[i]\n",
    "            m2 = model_pool_minus_one[j]\n",
    "            sum_q+=Q(df, y, m1, m2)\n",
    "    return 1-sum_q/len(model_pool_minus_one)\n",
    "\n",
    "def compute_difference(model_pool, df, y):\n",
    "    vals_q = {}\n",
    "    for i in model_pool:\n",
    "        model_pool_temp = model_pool.copy()\n",
    "        model_pool_temp.remove(i)\n",
    "        k = json.dumps(i)\n",
    "        vals_q[k] = compute_divS(model_pool_temp, df, y)\n",
    "    to_remove = min(vals_q, key=vals_q.get)\n",
    "    to_remove = json.loads(to_remove)\n",
    "    #print('removing ', to_remove)\n",
    "    model_pool.remove(to_remove)\n",
    "    return model_pool\n",
    "\n",
    "def Q(df, y, m1, m2):\n",
    "    p1 = predict(m1, df)\n",
    "    p2 = predict(m2, df)\n",
    "    \n",
    "    # matrice des resultats\n",
    "    r = pd.DataFrame()\n",
    "    r['y'] = y\n",
    "    r['m1'] = p1\n",
    "    r['m2'] = p2\n",
    "    r['m1'] = (r['y']==r['m1']).astype(int)\n",
    "    r['m2'] = (r['y']==r['m2']).astype(int)\n",
    "    N00 = max(1, len(r[(r.m1==0) & (r.m2==0)]))\n",
    "    N10 = max(1, len(r[(r.m1==1) & (r.m2==0)]))\n",
    "    N01 = max(1, len(r[(r.m1==0) & (r.m2==1)]))\n",
    "    N11 = max(1, len(r[(r.m1==1) & (r.m2==1)]))\n",
    "    return (N00*N11-N01*N10)/(N00*N11+N01*N10)\n",
    "\n",
    "class dtel:\n",
    "    def __init__(self, pool_size=4, max_depth=4, min_sample_split=5):\n",
    "        self.pool_size=pool_size\n",
    "        self.pool = []\n",
    "        self.new_pool = []\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.model_pool_weight = []\n",
    "    \n",
    "    def train_new_model(self, df, y):\n",
    "        df = pd.DataFrame(df)\n",
    "        df['target'] = y\n",
    "        df = df.to_numpy()\n",
    "         # update all models\n",
    "        self.new_pool = copy.deepcopy(self.pool[:])\n",
    "        \n",
    "        self.temp_pool = []\n",
    "        for tr in self.new_pool:\n",
    "            tree = copy.deepcopy(tr.copy())\n",
    "            self.temp_pool.append(adapt_tree(tree, pd.DataFrame(df), max_depth=self.max_depth,\n",
    "                                             min_sample_size=self.min_sample_split))\n",
    "        # train new model\n",
    "        self.temp_pool.append(build_tree(df, self.max_depth, self.min_sample_split))\n",
    "        # compute weights\n",
    "        self.model_pool_weight = mse_i_t(self.temp_pool, df)\n",
    "        \n",
    "    def update_pool(self, df, y):\n",
    "        if len(self.temp_pool) > self.pool_size:\n",
    "            self.pool = compute_difference(self.pool, df, y)\n",
    "        else :\n",
    "            self.new_pool.extend([self.temp_pool[-1]])\n",
    "            self.pool = list(self.new_pool[:])\n",
    "\n",
    "    def predict(self, df):\n",
    "        pred = np.array(predict(self.temp_pool[0], df))*self.model_pool_weight[0]\n",
    "        for i in range(1, len(self.temp_pool)):\n",
    "            pred = pred+np.array(predict(self.temp_pool[i], df))*self.model_pool_weight[i]\n",
    "            i+=1\n",
    "        pred = pred/sum(self.model_pool_weight)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n",
      "[{'index': 3, 'value': 1.0, 'left': {'index': 0, 'value': 0.902272858895897, 'left': {'index': 0, 'value': 0.6994889014244758, 'left': 0.0, 'right': 0.0}, 'right': 0.0}, 'right': {'index': 0, 'value': 0.8841293001637827, 'left': {'index': 0, 'value': 0.16186945186218826, 'left': 1.0, 'right': 1.0}, 'right': 1.0}}]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test performance vs decision tree\n",
    "dt = dtel(max_depth=3)\n",
    "dataset = np.random.random(size = [15, 4])\n",
    "df = pd.DataFrame(dataset)\n",
    "df[3] = (df[3]< 0.45).astype(int)\n",
    "y = df[3]\n",
    "\n",
    "dt.train_new_model(df, y)\n",
    "#print(dt.temp_pool)\n",
    "print(dt.pool)\n",
    "print()\n",
    "dt.update_pool(df, y)\n",
    "#print(dt.temp_pool)\n",
    "print(dt.pool)\n",
    "print()\n",
    "dt.predict(df)\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(df, y)\n",
    "\n",
    "clf.predict(df)==dt.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = pd.read_csv('../dataset/electricity-normalized.csv')\n",
    "del X['date']\n",
    "y = (X['class']=='UP').astype(int)\n",
    "del X['class']\n",
    "\n",
    "\n",
    "dt = dtel(max_depth=3, pool_size=25)\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "results = []\n",
    "preds = []\n",
    "correct_cnt, samples = 0, 0\n",
    "\n",
    "df_init, y_init = X[X.index<batch_size].to_numpy(), y[y.index<batch_size]\n",
    "dt.train_new_model(df_init, y_init)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for i in range(batch_size, len(X)-2*batch_size, batch_size):\n",
    "    X_, y_ = X[(X.index >= i) & (X.index < i+batch_size)] , y[(y.index >= i) & (y.index < i+batch_size)]\n",
    "    y_pred = dt.predict(X_)\n",
    "    correct_cnt += sum(y_ == y_pred)\n",
    "    preds.extend(y_pred)\n",
    "    results.append(sum(y_ == y_pred)/batch_size)\n",
    "    \n",
    "    dt.train_new_model(X_, y_)\n",
    "    dt.update_pool(X_, y_)\n",
    "print('Learn++.NSE classifier accuracy: {}'.format(correct_cnt / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(batch_size, len(X)-2*batch_size, batch_size):\n",
    "    X_, y_ = X[(X.index >= i) & (X.index < i+batch_size)] , y[(y.index >= i) & (y.index < i+batch_size)]\n",
    "    yy.extend(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = pd.DataFrame(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "P['preds'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "P['preds1'] = (P['preds']>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7513863636363637"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(P[0]==P['preds1'])/44000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
