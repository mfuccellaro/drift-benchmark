{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-multiflow --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1607422801.6473866"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class dtel:\\n    def __init__(self, pool_size=4):\\n        self.pool_size=pool_size\\n        self.pool = []\\n        self.new_pool = []\\n        self.model_pool_weight = []\\n    \\n    def train_new_model(self, df, y):\\n         # update all models\\n        self.new_pool = copy.deepcopy(self.pool[:])\\n        \\n        self.temp_pool = []\\n        # for all models\\n        #print('updating models')\\n        \\n        for m in self.new_pool:\\n            start = time.time()\\n            model = copy.deepcopy(m)\\n            #freeze all layers except last one\\n            for i in model.layers[:-1]:\\n                i.trainable = False\\n            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\n            model.fit(df, y, epochs=20, batch_size=100, verbose=0)\\n            print('retrain lasted ', time.time()-start)\\n            self.temp_pool.append(model)\\n            keras.backend.clear_session()\\n            \\n        # train new model\\n        #print('training models')\\n        start = time.time()\\n        new_model = create_model()\\n        new_model.fit(df, y, epochs=20, batch_size=100, verbose=0)\\n        print('train from scratch lasted ', time.time()-start)\\n        self.temp_pool.append(new_model)\\n        keras.backend.clear_session()\\n        # compute weights\\n        self.model_pool_weight = mse_i_t(self.temp_pool, df)\\n        \\n        \\n    def update_pool(self, df, y):\\n        if len(self.temp_pool) > self.pool_size:\\n            self.pool = compute_difference(self.pool, df, y)\\n        else :\\n            self.new_pool.extend([self.temp_pool[-1]])\\n            self.pool = list(self.new_pool[:])\\n\\n    def predict(self, df):\\n        y_prob = self.temp_pool[0].predict(df)\\n        pred = np.array([np.argmax(y_prob[i]) for i in range(len(y_prob))])*self.model_pool_weight[0]\\n        \\n        for i in range(1, len(self.temp_pool)):\\n            y_prob = self.temp_pool[0].predict(df)\\n            pred = pred+np.array([np.argmax(y_prob[j]) for j in range(len(y_prob))])*self.model_pool_weight[i]\\n\\n        pred = pred/sum(self.model_pool_weight)\\n        return pred\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class dtel:\n",
    "    def __init__(self, pool_size=4):\n",
    "        self.pool_size=pool_size\n",
    "        self.pool = []\n",
    "        self.new_pool = []\n",
    "        self.model_pool_weight = []\n",
    "    \n",
    "    def train_new_model(self, df, y):\n",
    "         # update all models\n",
    "        self.new_pool = copy.deepcopy(self.pool[:])\n",
    "        \n",
    "        self.temp_pool = []\n",
    "        # for all models\n",
    "        #print('updating models')\n",
    "        \n",
    "        for m in self.new_pool:\n",
    "            start = time.time()\n",
    "            model = copy.deepcopy(m)\n",
    "            #freeze all layers except last one\n",
    "            for i in model.layers[:-1]:\n",
    "                i.trainable = False\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            model.fit(df, y, epochs=20, batch_size=100, verbose=0)\n",
    "            print('retrain lasted ', time.time()-start)\n",
    "            self.temp_pool.append(model)\n",
    "            keras.backend.clear_session()\n",
    "            \n",
    "        # train new model\n",
    "        #print('training models')\n",
    "        start = time.time()\n",
    "        new_model = create_model()\n",
    "        new_model.fit(df, y, epochs=20, batch_size=100, verbose=0)\n",
    "        print('train from scratch lasted ', time.time()-start)\n",
    "        self.temp_pool.append(new_model)\n",
    "        keras.backend.clear_session()\n",
    "        # compute weights\n",
    "        self.model_pool_weight = mse_i_t(self.temp_pool, df)\n",
    "        \n",
    "        \n",
    "    def update_pool(self, df, y):\n",
    "        if len(self.temp_pool) > self.pool_size:\n",
    "            self.pool = compute_difference(self.pool, df, y)\n",
    "        else :\n",
    "            self.new_pool.extend([self.temp_pool[-1]])\n",
    "            self.pool = list(self.new_pool[:])\n",
    "\n",
    "    def predict(self, df):\n",
    "        y_prob = self.temp_pool[0].predict(df)\n",
    "        pred = np.array([np.argmax(y_prob[i]) for i in range(len(y_prob))])*self.model_pool_weight[0]\n",
    "        \n",
    "        for i in range(1, len(self.temp_pool)):\n",
    "            y_prob = self.temp_pool[0].predict(df)\n",
    "            pred = pred+np.array([np.argmax(y_prob[j]) for j in range(len(y_prob))])*self.model_pool_weight[i]\n",
    "\n",
    "        pred = pred/sum(self.model_pool_weight)\n",
    "        return pred'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "import time as t\n",
    "import time\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from skmultiflow.data import SEAGenerator as sea\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    keras.backend.clear_session()\n",
    "    num_classes = 2\n",
    "    num_h = 5\n",
    "    model = Sequential()\n",
    "    l1 = Flatten(input_shape=(1, 3))\n",
    "    model.add(l1)\n",
    "    l2 = Dense(num_h, activation='relu', kernel_initializer='normal')\n",
    "    model.add(l2)\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_initializer='normal'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_i_t(model_pool, df):\n",
    "    start = time.time()\n",
    "    w = []\n",
    "    for i in range(len(model_pool)-1):\n",
    "        m = keras.models.load_model(model_pool[i])\n",
    "        w.append(compute_weights(m, df))\n",
    "    # last model is of weight highest\n",
    "    w.append(1)\n",
    "    print('call to mse_i_t lasted', time.time()-start)\n",
    "    return w\n",
    "\n",
    "def compute_weights(model, df):\n",
    "    start = time.time()\n",
    "    eps = 1\n",
    "    try:\n",
    "        df = df.to_numpy()\n",
    "    except:\n",
    "        pass\n",
    "    y = list(row[-1] for row in df)\n",
    "    y_prob = model.predict(df)\n",
    "    y_pred = [np.argmax(y_prob[i]) for i in range(len(y_prob))]\n",
    "    print('call to compute_weights lasted', time.time()-start)\n",
    "    return 1/(sum((pd.DataFrame(y_pred)[0]-pd.DataFrame(y)[1])**2)/len(df)+eps)\n",
    "\n",
    "\n",
    "\n",
    "def compute_divS(model_pool_minus_one, df, y):\n",
    "    start = time.time()\n",
    "    sum_q = 0\n",
    "    for i in range(len(model_pool_minus_one)):\n",
    "        for j in range(i-1):\n",
    "            m1 = model_pool_minus_one[i]\n",
    "            m2 = model_pool_minus_one[j]\n",
    "            sum_q+=Q(df, y, m1, m2)\n",
    "    print('call to compute_divS lasted', time.time()-start)\n",
    "    return 1-sum_q/len(model_pool_minus_one)\n",
    "\n",
    "'''def compute_difference(model_pool, df, y):\n",
    "    start = time.time()\n",
    "    vals_q = {}\n",
    "    for i in model_pool:\n",
    "        model_pool_temp = model_pool.copy()\n",
    "        model_pool_temp.remove(i)\n",
    "        vals_q[i] = compute_divS(model_pool_temp, df, y)\n",
    "    to_remove = min(vals_q, key=vals_q.get)\n",
    "    #print('removing ', to_remove)\n",
    "    model_pool.remove(to_remove)\n",
    "    print('call to compute_difference lasted', time.time()-start)\n",
    "    return model_pool'''\n",
    "\n",
    "def compute_difference(model_pool_path, df, y):\n",
    "    start = time.time()\n",
    "    vals_q = {}\n",
    "    for i in model_pool_temp_path:\n",
    "        \n",
    "        model_pool_temp_path = copy.deepcopy(model_pool_temp_path)\n",
    "        model_pool_temp_path.remove(i)\n",
    "        model_pool_temp = [keras.models.load_model(i) for i in model_pool_temp_path]\n",
    "        vals_q[i] = compute_divS(model_pool_temp, df, y)\n",
    "    to_remove = min(vals_q, key=vals_q.get)\n",
    "    #print('removing ', to_remove)\n",
    "    model_pool.remove(to_remove)\n",
    "    print('call to compute_difference lasted', time.time()-start)\n",
    "    return model_pool\n",
    "\n",
    "def Q(df, y, m1, m2):\n",
    "    start = time.time()\n",
    "    y_prob = m1.predict(df)\n",
    "    p1 = [np.argmax(y_prob[i]) for i in range(len(y_prob))]\n",
    "    \n",
    "    y_prob = m2.predict(df)\n",
    "    p2 = [np.argmax(y_prob[i]) for i in range(len(y_prob))]\n",
    "    \n",
    "    # matrice des resultats\n",
    "    r = pd.DataFrame()\n",
    "    r['y'] = y\n",
    "    r['m1'] = p1\n",
    "    r['m2'] = p2\n",
    "    r['m1'] = (r['y']==r['m1']).astype(int)\n",
    "    r['m2'] = (r['y']==r['m2']).astype(int)\n",
    "    N00 = max(1, len(r[(r.m1==0) & (r.m2==0)]))\n",
    "    N10 = max(1, len(r[(r.m1==1) & (r.m2==0)]))\n",
    "    N01 = max(1, len(r[(r.m1==0) & (r.m2==1)]))\n",
    "    N11 = max(1, len(r[(r.m1==1) & (r.m2==1)]))\n",
    "    print('call to Q lasted', time.time()-start)\n",
    "    return (N00*N11-N01*N10)/(N00*N11+N01*N10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas_g = [10, 8, 6, 8, 10, 12, 14, 12]\n",
    "\n",
    "def class_function(df, theta, noise=True):\n",
    "    y = ((df[0]+df[1])<=theta).astype(int)\n",
    "    if noise:\n",
    "        for i in y.sample(frac=0.1).index:\n",
    "            y.loc[i] = random.randint(0, 1)\n",
    "    return y\n",
    "\n",
    "def buid_set(size, generator, theta):\n",
    "    X, Y, X_ = [], [], []\n",
    "    for i in range(size):\n",
    "        x, y = generator.next_sample()\n",
    "        X_.append(x[0])\n",
    "        X.append(x)\n",
    "        #Y.append(float(y[0]))\n",
    "\n",
    "    Y = class_function(pd.DataFrame(X_), theta, noise=True)\n",
    "    Y = pd.get_dummies(pd.Series(Y)).to_numpy()\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "generator = sea()\n",
    "\n",
    "\n",
    "X_train, y_train = buid_set(10000, generator, 10)\n",
    "X_test, y_test = buid_set(1000, generator, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dtel:\n",
    "    def __init__(self, pool_size=4):\n",
    "        self.pool_size=pool_size\n",
    "        self.pool = []\n",
    "        self.new_pool = []\n",
    "        self.model_pool_weight = []\n",
    "        self.model_pool_paths = []\n",
    "    \n",
    "    def train_new_model(self, df, y):\n",
    "        # update all models\n",
    "        self.new_pool = copy.deepcopy(self.pool[:])\n",
    "        \n",
    "        \n",
    "        self.temp_pool = []\n",
    "        # for all models\n",
    "        #print('updating models')\n",
    "        \n",
    "        for path in self.model_pool_paths:\n",
    "            start = time.time()\n",
    "            model = keras.models.load_model(path)\n",
    "            #freeze all layers except last one\n",
    "            for i in model.layers[:-1]:\n",
    "                i.trainable = False\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            \n",
    "            model.fit(df, y, epochs=20, batch_size=100, verbose=0)\n",
    "            print('retrain lasted ', time.time()-start)\n",
    "            self.temp_pool.append(path)\n",
    "            \n",
    "        # train new model\n",
    "        #print('training models')\n",
    "        start = time.time()\n",
    "        new_model = create_model()\n",
    "        new_model.fit(df, y, epochs=20, batch_size=100, verbose=0)\n",
    "        print('train from scratch lasted ', time.time()-start)\n",
    "        m_name = 'models/model-'+str(random.randint(0, 100000000))\n",
    "        new_model.save(m_name)\n",
    "        keras.backend.clear_session()\n",
    "        self.model_pool_paths.append(m_name)\n",
    "        self.temp_pool.append(m_name)\n",
    "        # compute weights\n",
    "        self.model_pool_weight = mse_i_t(self.temp_pool, df)\n",
    "        \n",
    "    def update_pool(self, df, y):\n",
    "        if len(self.temp_pool) > self.pool_size:\n",
    "            self.pool = compute_difference(self.pool, df, y)\n",
    "        else :\n",
    "            self.new_pool.extend([self.temp_pool[-1]])\n",
    "            self.pool = list(self.new_pool[:])\n",
    "\n",
    "    def predict(self, df):\n",
    "        y_prob = keras.models.load_model(self.temp_pool[0]).predict(df)\n",
    "        pred = np.array([np.argmax(y_prob[i]) for i in range(len(y_prob))])*self.model_pool_weight[0]\n",
    "        \n",
    "        for i in range(1, len(self.temp_pool)):\n",
    "            y_prob = keras.models.load_model(self.temp_pool[0]).predict(df)\n",
    "            pred = pred+np.array([np.argmax(y_prob[j]) for j in range(len(y_prob))])*self.model_pool_weight[i]\n",
    "\n",
    "        pred = pred/sum(self.model_pool_weight)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = dt.pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44347575, 0.55652434],\n",
       "       [0.6083661 , 0.39163393],\n",
       "       [0.4267288 , 0.5732712 ],\n",
       "       [0.49706712, 0.50293285],\n",
       "       [0.6282034 , 0.37179664],\n",
       "       [0.33017904, 0.66982096],\n",
       "       [0.5143756 , 0.48562434],\n",
       "       [0.50209856, 0.49790147],\n",
       "       [0.45032465, 0.54967535],\n",
       "       [0.411067  , 0.58893305],\n",
       "       [0.41390935, 0.5860906 ],\n",
       "       [0.59240204, 0.40759796],\n",
       "       [0.53431225, 0.46568772],\n",
       "       [0.62512136, 0.37487862],\n",
       "       [0.52128917, 0.4787108 ],\n",
       "       [0.50159115, 0.49840888],\n",
       "       [0.5757906 , 0.42420945],\n",
       "       [0.57401043, 0.42598963],\n",
       "       [0.5308843 , 0.46911573],\n",
       "       [0.59802055, 0.40197948],\n",
       "       [0.52030057, 0.4796995 ],\n",
       "       [0.43330497, 0.56669503],\n",
       "       [0.57886595, 0.4211341 ],\n",
       "       [0.6246015 , 0.37539846],\n",
       "       [0.52389055, 0.47610942],\n",
       "       [0.55535036, 0.44464964],\n",
       "       [0.6374854 , 0.36251467],\n",
       "       [0.5692354 , 0.43076462],\n",
       "       [0.48996332, 0.51003677],\n",
       "       [0.5105023 , 0.48949775],\n",
       "       [0.4021059 , 0.597894  ],\n",
       "       [0.5978824 , 0.40211767],\n",
       "       [0.48119217, 0.51880777],\n",
       "       [0.635944  , 0.36405602],\n",
       "       [0.5219164 , 0.4780836 ],\n",
       "       [0.5581684 , 0.44183156],\n",
       "       [0.35056153, 0.6494384 ],\n",
       "       [0.47950825, 0.5204917 ],\n",
       "       [0.3635617 , 0.63643825],\n",
       "       [0.5632449 , 0.4367551 ],\n",
       "       [0.33724147, 0.6627585 ],\n",
       "       [0.60048753, 0.3995124 ],\n",
       "       [0.54393005, 0.45606992],\n",
       "       [0.4490074 , 0.5509926 ],\n",
       "       [0.5756582 , 0.42434177],\n",
       "       [0.58404195, 0.41595802],\n",
       "       [0.5226933 , 0.47730675],\n",
       "       [0.54737484, 0.45262516],\n",
       "       [0.5967927 , 0.4032073 ],\n",
       "       [0.58801204, 0.41198793],\n",
       "       [0.585855  , 0.414145  ],\n",
       "       [0.52456486, 0.47543523],\n",
       "       [0.40960017, 0.59039974],\n",
       "       [0.4520007 , 0.5479992 ],\n",
       "       [0.3619351 , 0.6380648 ],\n",
       "       [0.5823301 , 0.4176699 ],\n",
       "       [0.5508702 , 0.44912988],\n",
       "       [0.4203534 , 0.5796466 ],\n",
       "       [0.54421234, 0.4557876 ],\n",
       "       [0.41987717, 0.5801228 ],\n",
       "       [0.5311284 , 0.46887165],\n",
       "       [0.483655  , 0.5163451 ],\n",
       "       [0.4569813 , 0.54301876],\n",
       "       [0.55934286, 0.44065717],\n",
       "       [0.41099057, 0.58900946],\n",
       "       [0.6202462 , 0.3797538 ],\n",
       "       [0.59956014, 0.40043986],\n",
       "       [0.55140275, 0.4485972 ],\n",
       "       [0.4191164 , 0.58088356],\n",
       "       [0.47335732, 0.5266427 ],\n",
       "       [0.5306419 , 0.46935815],\n",
       "       [0.53618234, 0.4638177 ],\n",
       "       [0.5159457 , 0.48405436],\n",
       "       [0.57509047, 0.4249095 ],\n",
       "       [0.5201916 , 0.4798084 ],\n",
       "       [0.62598836, 0.37401155],\n",
       "       [0.43740174, 0.5625982 ],\n",
       "       [0.49622577, 0.5037742 ],\n",
       "       [0.5985601 , 0.40143996],\n",
       "       [0.62535536, 0.3746446 ],\n",
       "       [0.5787021 , 0.42129788],\n",
       "       [0.5343695 , 0.4656305 ],\n",
       "       [0.5247537 , 0.47524634],\n",
       "       [0.5875578 , 0.41244218],\n",
       "       [0.5093509 , 0.49064916],\n",
       "       [0.5350501 , 0.46494982],\n",
       "       [0.49605328, 0.5039467 ],\n",
       "       [0.5480095 , 0.45199046],\n",
       "       [0.56560117, 0.43439886],\n",
       "       [0.54805857, 0.45194143],\n",
       "       [0.5901953 , 0.40980473],\n",
       "       [0.57935303, 0.420647  ],\n",
       "       [0.5626676 , 0.43733236],\n",
       "       [0.6424759 , 0.3575241 ],\n",
       "       [0.5963937 , 0.40360627],\n",
       "       [0.5648001 , 0.43519983],\n",
       "       [0.5519215 , 0.4480785 ],\n",
       "       [0.5298453 , 0.47015476],\n",
       "       [0.43562678, 0.56437325],\n",
       "       [0.5749781 , 0.42502186],\n",
       "       [0.6329483 , 0.36705178],\n",
       "       [0.56372243, 0.43627754],\n",
       "       [0.50470215, 0.4952978 ],\n",
       "       [0.5271168 , 0.47288328],\n",
       "       [0.35371357, 0.6462864 ],\n",
       "       [0.41790235, 0.58209765],\n",
       "       [0.6411136 , 0.35888642],\n",
       "       [0.39709547, 0.60290444],\n",
       "       [0.47353145, 0.5264686 ],\n",
       "       [0.61738896, 0.38261104],\n",
       "       [0.6419306 , 0.35806945],\n",
       "       [0.37937903, 0.62062097],\n",
       "       [0.59784317, 0.4021568 ],\n",
       "       [0.5712914 , 0.42870852],\n",
       "       [0.52847075, 0.4715292 ],\n",
       "       [0.58197945, 0.41802052],\n",
       "       [0.62267107, 0.37732887],\n",
       "       [0.490702  , 0.509298  ],\n",
       "       [0.5502233 , 0.44977662],\n",
       "       [0.4919163 , 0.5080837 ],\n",
       "       [0.5178415 , 0.48215845],\n",
       "       [0.51991844, 0.48008153],\n",
       "       [0.5128988 , 0.48710123],\n",
       "       [0.58727163, 0.4127284 ],\n",
       "       [0.56259084, 0.4374091 ],\n",
       "       [0.5624768 , 0.4375232 ],\n",
       "       [0.55148417, 0.44851583],\n",
       "       [0.56195533, 0.43804467],\n",
       "       [0.6226121 , 0.37738788],\n",
       "       [0.44550678, 0.55449325],\n",
       "       [0.44971102, 0.5502889 ],\n",
       "       [0.563382  , 0.43661797],\n",
       "       [0.42673773, 0.5732623 ],\n",
       "       [0.58739805, 0.41260192],\n",
       "       [0.59013444, 0.40986556],\n",
       "       [0.5244855 , 0.4755145 ],\n",
       "       [0.49704295, 0.5029571 ],\n",
       "       [0.45220017, 0.54779977],\n",
       "       [0.4571642 , 0.5428357 ],\n",
       "       [0.5256906 , 0.47430944],\n",
       "       [0.49227464, 0.50772536],\n",
       "       [0.53895915, 0.46104088],\n",
       "       [0.5746865 , 0.42531344],\n",
       "       [0.634876  , 0.36512405],\n",
       "       [0.36952853, 0.6304714 ],\n",
       "       [0.3745717 , 0.62542826],\n",
       "       [0.6265575 , 0.37344253],\n",
       "       [0.41703194, 0.582968  ],\n",
       "       [0.63009095, 0.36990902],\n",
       "       [0.34676787, 0.6532321 ],\n",
       "       [0.48775035, 0.5122496 ],\n",
       "       [0.54987186, 0.45012814],\n",
       "       [0.41990888, 0.5800912 ],\n",
       "       [0.63717103, 0.362829  ],\n",
       "       [0.5967521 , 0.40324792],\n",
       "       [0.4847193 , 0.5152807 ],\n",
       "       [0.3563955 , 0.64360446],\n",
       "       [0.59088784, 0.40911213],\n",
       "       [0.3660285 , 0.6339715 ],\n",
       "       [0.31512475, 0.68487525],\n",
       "       [0.4739994 , 0.52600056],\n",
       "       [0.60292965, 0.39707026],\n",
       "       [0.45973393, 0.5402661 ],\n",
       "       [0.56617063, 0.43382937],\n",
       "       [0.44334042, 0.5566595 ],\n",
       "       [0.5925264 , 0.4074736 ],\n",
       "       [0.5860533 , 0.41394672],\n",
       "       [0.4952566 , 0.5047434 ],\n",
       "       [0.45209718, 0.5479028 ],\n",
       "       [0.48618045, 0.5138195 ],\n",
       "       [0.5759218 , 0.4240782 ],\n",
       "       [0.523827  , 0.47617292],\n",
       "       [0.5125087 , 0.4874913 ],\n",
       "       [0.5724256 , 0.42757443],\n",
       "       [0.5353439 , 0.46465608],\n",
       "       [0.41540793, 0.58459204],\n",
       "       [0.5861136 , 0.41388643],\n",
       "       [0.5657539 , 0.43424612],\n",
       "       [0.5791206 , 0.4208794 ],\n",
       "       [0.40932772, 0.59067225],\n",
       "       [0.40179896, 0.59820104],\n",
       "       [0.5664186 , 0.4335814 ],\n",
       "       [0.47793987, 0.52206016],\n",
       "       [0.32934794, 0.6706521 ],\n",
       "       [0.50337344, 0.49662656],\n",
       "       [0.51895875, 0.48104122],\n",
       "       [0.58937347, 0.41062656],\n",
       "       [0.5708414 , 0.4291586 ],\n",
       "       [0.387814  , 0.61218596],\n",
       "       [0.38462377, 0.61537623],\n",
       "       [0.51769227, 0.48230773],\n",
       "       [0.4992195 , 0.5007804 ],\n",
       "       [0.49028814, 0.5097119 ],\n",
       "       [0.37743858, 0.62256145],\n",
       "       [0.52808696, 0.47191304],\n",
       "       [0.40940318, 0.5905968 ],\n",
       "       [0.63065743, 0.3693425 ],\n",
       "       [0.49876925, 0.5012308 ],\n",
       "       [0.5368022 , 0.46319783],\n",
       "       [0.50677526, 0.4932247 ],\n",
       "       [0.49141234, 0.5085877 ],\n",
       "       [0.57723606, 0.42276397],\n",
       "       [0.46085498, 0.539145  ],\n",
       "       [0.6348875 , 0.36511242],\n",
       "       [0.40598562, 0.5940144 ],\n",
       "       [0.5122554 , 0.48774463],\n",
       "       [0.6032408 , 0.39675918],\n",
       "       [0.539277  , 0.46072298],\n",
       "       [0.47791746, 0.52208257],\n",
       "       [0.4746664 , 0.5253336 ],\n",
       "       [0.58561873, 0.41438124],\n",
       "       [0.54343957, 0.4565605 ],\n",
       "       [0.55300367, 0.44699633],\n",
       "       [0.5456587 , 0.45434132],\n",
       "       [0.36859965, 0.6314004 ],\n",
       "       [0.5198969 , 0.48010305],\n",
       "       [0.6284149 , 0.3715851 ],\n",
       "       [0.41901845, 0.58098155],\n",
       "       [0.5117811 , 0.48821884],\n",
       "       [0.5446589 , 0.45534104],\n",
       "       [0.38400215, 0.61599785],\n",
       "       [0.62332183, 0.3766782 ],\n",
       "       [0.52856   , 0.47144002],\n",
       "       [0.5993416 , 0.4006584 ],\n",
       "       [0.56132865, 0.43867135],\n",
       "       [0.55446863, 0.44553137],\n",
       "       [0.6250704 , 0.37492952],\n",
       "       [0.60561365, 0.39438632],\n",
       "       [0.47590566, 0.5240944 ],\n",
       "       [0.3986969 , 0.60130304],\n",
       "       [0.619461  , 0.380539  ],\n",
       "       [0.6341609 , 0.36583918],\n",
       "       [0.6060688 , 0.39393115],\n",
       "       [0.43905506, 0.5609449 ],\n",
       "       [0.4984131 , 0.5015869 ],\n",
       "       [0.5644512 , 0.43554872],\n",
       "       [0.49897772, 0.5010222 ],\n",
       "       [0.62935776, 0.37064227],\n",
       "       [0.6391448 , 0.3608552 ],\n",
       "       [0.55599487, 0.44400513],\n",
       "       [0.44584325, 0.55415666],\n",
       "       [0.6009972 , 0.39900276],\n",
       "       [0.5640522 , 0.43594778],\n",
       "       [0.47767305, 0.52232695],\n",
       "       [0.59203726, 0.40796268],\n",
       "       [0.6233892 , 0.37661085],\n",
       "       [0.5721861 , 0.42781392],\n",
       "       [0.52756184, 0.47243813],\n",
       "       [0.6128933 , 0.38710678],\n",
       "       [0.6225738 , 0.37742615],\n",
       "       [0.6408883 , 0.35911164],\n",
       "       [0.59721494, 0.402785  ],\n",
       "       [0.47566888, 0.5243311 ],\n",
       "       [0.5496307 , 0.45036933],\n",
       "       [0.34661353, 0.6533864 ],\n",
       "       [0.3331927 , 0.66680723],\n",
       "       [0.6339696 , 0.36603034],\n",
       "       [0.44957387, 0.55042607],\n",
       "       [0.48486072, 0.51513934],\n",
       "       [0.59447134, 0.40552863],\n",
       "       [0.56761414, 0.43238592],\n",
       "       [0.60879487, 0.39120516],\n",
       "       [0.5440187 , 0.45598137],\n",
       "       [0.44891995, 0.55108   ],\n",
       "       [0.47333118, 0.5266688 ],\n",
       "       [0.4919739 , 0.50802606],\n",
       "       [0.3717596 , 0.6282404 ],\n",
       "       [0.522545  , 0.477455  ],\n",
       "       [0.6449345 , 0.3550655 ],\n",
       "       [0.56496924, 0.43503076],\n",
       "       [0.57523483, 0.42476523],\n",
       "       [0.58343965, 0.41656038],\n",
       "       [0.59784365, 0.40215626],\n",
       "       [0.5139588 , 0.48604116],\n",
       "       [0.44668835, 0.55331165],\n",
       "       [0.46961424, 0.5303858 ],\n",
       "       [0.6265125 , 0.37348744],\n",
       "       [0.5034086 , 0.49659133],\n",
       "       [0.48696166, 0.5130383 ],\n",
       "       [0.405395  , 0.594605  ],\n",
       "       [0.5359701 , 0.4640299 ],\n",
       "       [0.5066213 , 0.49337876],\n",
       "       [0.48947868, 0.51052135],\n",
       "       [0.5404465 , 0.45955348],\n",
       "       [0.41907504, 0.580925  ],\n",
       "       [0.55624247, 0.4437576 ],\n",
       "       [0.39069957, 0.6093004 ],\n",
       "       [0.5387534 , 0.46124658],\n",
       "       [0.61193347, 0.3880665 ],\n",
       "       [0.34326565, 0.6567344 ],\n",
       "       [0.4061309 , 0.5938691 ],\n",
       "       [0.5020815 , 0.49791855],\n",
       "       [0.4889673 , 0.5110327 ],\n",
       "       [0.5617945 , 0.4382055 ],\n",
       "       [0.33796757, 0.6620324 ],\n",
       "       [0.5787259 , 0.42127413],\n",
       "       [0.6304126 , 0.36958742],\n",
       "       [0.50219053, 0.49780947],\n",
       "       [0.46598548, 0.5340145 ],\n",
       "       [0.5757363 , 0.4242637 ],\n",
       "       [0.51919335, 0.48080662],\n",
       "       [0.41985163, 0.58014834],\n",
       "       [0.4879927 , 0.5120073 ],\n",
       "       [0.3556257 , 0.6443743 ],\n",
       "       [0.6220669 , 0.3779331 ],\n",
       "       [0.56294245, 0.43705752],\n",
       "       [0.4387367 , 0.56126326],\n",
       "       [0.5222123 , 0.4777877 ],\n",
       "       [0.42985454, 0.5701454 ],\n",
       "       [0.39289424, 0.6071057 ],\n",
       "       [0.590456  , 0.4095439 ],\n",
       "       [0.4883753 , 0.5116247 ],\n",
       "       [0.61952525, 0.38047475],\n",
       "       [0.5542039 , 0.44579613],\n",
       "       [0.58763903, 0.41236097],\n",
       "       [0.5222532 , 0.4777468 ],\n",
       "       [0.50196946, 0.49803057],\n",
       "       [0.6349919 , 0.36500815],\n",
       "       [0.55532247, 0.44467747],\n",
       "       [0.6265083 , 0.3734917 ],\n",
       "       [0.60786486, 0.3921352 ],\n",
       "       [0.6241347 , 0.37586522],\n",
       "       [0.32544756, 0.67455244],\n",
       "       [0.5392293 , 0.46077073],\n",
       "       [0.53216183, 0.46783817],\n",
       "       [0.41149497, 0.58850497],\n",
       "       [0.5926422 , 0.4073578 ],\n",
       "       [0.6171617 , 0.38283822],\n",
       "       [0.62175554, 0.3782445 ],\n",
       "       [0.3890723 , 0.6109277 ],\n",
       "       [0.6255381 , 0.37446183],\n",
       "       [0.51587474, 0.48412526],\n",
       "       [0.60527736, 0.3947227 ],\n",
       "       [0.61036927, 0.3896307 ],\n",
       "       [0.55138415, 0.44861585],\n",
       "       [0.56164956, 0.43835038],\n",
       "       [0.5048564 , 0.4951436 ],\n",
       "       [0.44850194, 0.55149806],\n",
       "       [0.54678446, 0.45321548],\n",
       "       [0.4844057 , 0.51559436],\n",
       "       [0.52877027, 0.4712297 ],\n",
       "       [0.518601  , 0.48139894],\n",
       "       [0.57283396, 0.42716601],\n",
       "       [0.33526912, 0.6647309 ],\n",
       "       [0.49184236, 0.5081577 ],\n",
       "       [0.4943692 , 0.50563073],\n",
       "       [0.40800402, 0.59199595],\n",
       "       [0.55394673, 0.44605327],\n",
       "       [0.53548664, 0.46451336],\n",
       "       [0.4095003 , 0.5904997 ],\n",
       "       [0.42974532, 0.5702546 ],\n",
       "       [0.5145217 , 0.48547828],\n",
       "       [0.5234222 , 0.4765778 ],\n",
       "       [0.49548024, 0.5045197 ],\n",
       "       [0.59701866, 0.40298137],\n",
       "       [0.5227388 , 0.4772612 ],\n",
       "       [0.55503476, 0.44496518],\n",
       "       [0.6313592 , 0.36864078],\n",
       "       [0.57380253, 0.4261975 ],\n",
       "       [0.6324775 , 0.36752248],\n",
       "       [0.51192653, 0.48807344],\n",
       "       [0.5201062 , 0.4798938 ],\n",
       "       [0.48548818, 0.5145118 ],\n",
       "       [0.58965266, 0.41034737],\n",
       "       [0.6115793 , 0.3884207 ],\n",
       "       [0.5428718 , 0.4571282 ],\n",
       "       [0.6558557 , 0.3441443 ],\n",
       "       [0.5628843 , 0.43711573],\n",
       "       [0.6196826 , 0.38031733],\n",
       "       [0.5573178 , 0.4426822 ],\n",
       "       [0.454682  , 0.54531807],\n",
       "       [0.4960512 , 0.5039488 ],\n",
       "       [0.6546605 , 0.3453395 ],\n",
       "       [0.53939646, 0.4606035 ],\n",
       "       [0.58643675, 0.41356325],\n",
       "       [0.6044787 , 0.39552128],\n",
       "       [0.43073142, 0.5692686 ],\n",
       "       [0.589712  , 0.41028795],\n",
       "       [0.6265264 , 0.37347353],\n",
       "       [0.49155462, 0.5084454 ],\n",
       "       [0.45880926, 0.54119074],\n",
       "       [0.46411476, 0.5358852 ],\n",
       "       [0.4451993 , 0.5548007 ],\n",
       "       [0.5031725 , 0.49682757],\n",
       "       [0.6574056 , 0.34259433],\n",
       "       [0.51904035, 0.48095965],\n",
       "       [0.57191247, 0.42808744],\n",
       "       [0.6144525 , 0.38554746],\n",
       "       [0.59489155, 0.40510848],\n",
       "       [0.5541747 , 0.44582525],\n",
       "       [0.41421545, 0.5857845 ],\n",
       "       [0.5387387 , 0.4612613 ],\n",
       "       [0.63617367, 0.36382633],\n",
       "       [0.5172075 , 0.4827925 ],\n",
       "       [0.45223555, 0.5477644 ],\n",
       "       [0.44315735, 0.5568427 ],\n",
       "       [0.43122616, 0.56877387],\n",
       "       [0.4030662 , 0.59693384],\n",
       "       [0.51793665, 0.4820633 ],\n",
       "       [0.5759103 , 0.4240897 ],\n",
       "       [0.51112336, 0.4888766 ],\n",
       "       [0.47108284, 0.5289172 ],\n",
       "       [0.5000679 , 0.49993205],\n",
       "       [0.5295743 , 0.47042567],\n",
       "       [0.46233842, 0.5376615 ],\n",
       "       [0.6114565 , 0.3885434 ],\n",
       "       [0.5910391 , 0.4089609 ],\n",
       "       [0.5645684 , 0.43543163],\n",
       "       [0.50949657, 0.49050346],\n",
       "       [0.5326027 , 0.46739724],\n",
       "       [0.54276735, 0.45723274],\n",
       "       [0.647182  , 0.35281795],\n",
       "       [0.52537984, 0.47462007],\n",
       "       [0.61901116, 0.38098884],\n",
       "       [0.6280598 , 0.37194023],\n",
       "       [0.5656823 , 0.43431765],\n",
       "       [0.5068141 , 0.49318588],\n",
       "       [0.5367416 , 0.46325845],\n",
       "       [0.62271756, 0.37728244],\n",
       "       [0.55277026, 0.44722977],\n",
       "       [0.47647217, 0.52352774],\n",
       "       [0.4435752 , 0.5564248 ],\n",
       "       [0.51518553, 0.48481447],\n",
       "       [0.5572987 , 0.44270128],\n",
       "       [0.6394681 , 0.36053187],\n",
       "       [0.5131677 , 0.48683235],\n",
       "       [0.58079433, 0.4192057 ],\n",
       "       [0.34188762, 0.6581123 ],\n",
       "       [0.51025844, 0.4897416 ],\n",
       "       [0.5322951 , 0.4677049 ],\n",
       "       [0.48054188, 0.51945806],\n",
       "       [0.5486203 , 0.4513798 ],\n",
       "       [0.45972195, 0.5402781 ],\n",
       "       [0.6061546 , 0.39384538],\n",
       "       [0.5701096 , 0.4298904 ],\n",
       "       [0.46005207, 0.53994787],\n",
       "       [0.4998308 , 0.50016916],\n",
       "       [0.52315027, 0.47684973],\n",
       "       [0.3936941 , 0.6063059 ],\n",
       "       [0.5380888 , 0.4619112 ],\n",
       "       [0.42223036, 0.5777696 ],\n",
       "       [0.52773106, 0.4722689 ],\n",
       "       [0.5444917 , 0.45550835],\n",
       "       [0.5531036 , 0.4468964 ],\n",
       "       [0.58367664, 0.41632333],\n",
       "       [0.5096177 , 0.49038234],\n",
       "       [0.53125745, 0.46874258],\n",
       "       [0.5690572 , 0.43094283],\n",
       "       [0.58093774, 0.4190622 ],\n",
       "       [0.6300233 , 0.36997664],\n",
       "       [0.591417  , 0.40858302],\n",
       "       [0.59732234, 0.40267766],\n",
       "       [0.6502707 , 0.34972936],\n",
       "       [0.5631792 , 0.4368208 ],\n",
       "       [0.54738694, 0.4526131 ],\n",
       "       [0.4677058 , 0.53229415],\n",
       "       [0.51359653, 0.48640347],\n",
       "       [0.58542323, 0.41457674],\n",
       "       [0.4878422 , 0.5121578 ],\n",
       "       [0.55921584, 0.4407842 ],\n",
       "       [0.42323425, 0.5767657 ],\n",
       "       [0.6087811 , 0.39121896],\n",
       "       [0.5356654 , 0.46433458],\n",
       "       [0.5437448 , 0.45625526],\n",
       "       [0.51518154, 0.4848185 ],\n",
       "       [0.61811304, 0.3818869 ],\n",
       "       [0.59525496, 0.404745  ],\n",
       "       [0.6065045 , 0.39349544],\n",
       "       [0.41090694, 0.5890931 ],\n",
       "       [0.4262559 , 0.57374406],\n",
       "       [0.5715164 , 0.42848355],\n",
       "       [0.57911366, 0.42088628],\n",
       "       [0.36262456, 0.6373754 ],\n",
       "       [0.5628767 , 0.4371233 ],\n",
       "       [0.5416754 , 0.4583246 ],\n",
       "       [0.52163166, 0.47836834],\n",
       "       [0.5619953 , 0.4380046 ],\n",
       "       [0.62228334, 0.37771663],\n",
       "       [0.5658941 , 0.4341059 ],\n",
       "       [0.5567937 , 0.44320628],\n",
       "       [0.42768934, 0.5723106 ],\n",
       "       [0.47435257, 0.52564746],\n",
       "       [0.635537  , 0.36446297],\n",
       "       [0.5759374 , 0.42406255],\n",
       "       [0.37809148, 0.62190855],\n",
       "       [0.5690584 , 0.43094155],\n",
       "       [0.48700786, 0.51299214],\n",
       "       [0.5973419 , 0.40265808],\n",
       "       [0.51179564, 0.4882044 ],\n",
       "       [0.58067745, 0.41932255],\n",
       "       [0.44738764, 0.5526124 ],\n",
       "       [0.6137783 , 0.38622162],\n",
       "       [0.58785796, 0.41214195],\n",
       "       [0.3734529 , 0.6265471 ],\n",
       "       [0.5455712 , 0.45442873],\n",
       "       [0.38635153, 0.6136485 ],\n",
       "       [0.6487634 , 0.35123658],\n",
       "       [0.5322842 , 0.4677158 ],\n",
       "       [0.3663861 , 0.63361394],\n",
       "       [0.42547712, 0.5745228 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.sequential.Sequential at 0x7f7240436748>, 'model-53633328']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.temp_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model-21181371', 'model-53633328']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train from scratch lasted  1.1466691493988037\n",
      "call to mse_i_t lasted 3.337860107421875e-06\n",
      "1 0.482 1.997910499572754\n",
      "retrain lasted  1.818281650543213\n",
      "train from scratch lasted  1.1399996280670166\n",
      "call to compute_weights lasted 0.07353925704956055\n",
      "call to mse_i_t lasted 0.6964309215545654\n",
      "2 0.47 5.470816612243652\n",
      "retrain lasted  1.925886869430542\n",
      "retrain lasted  2.126603126525879\n",
      "train from scratch lasted  1.141930103302002\n",
      "call to compute_weights lasted 0.07512998580932617\n",
      "call to compute_weights lasted 0.08815789222717285\n",
      "call to mse_i_t lasted 1.4974651336669922\n",
      "3 0.506 9.787433862686157\n",
      "retrain lasted  2.4724528789520264\n",
      "retrain lasted  2.5817830562591553\n",
      "retrain lasted  2.8277230262756348\n",
      "train from scratch lasted  1.1543269157409668\n",
      "call to compute_weights lasted 0.07506251335144043\n",
      "call to compute_weights lasted 0.08895134925842285\n",
      "call to compute_weights lasted 0.10768008232116699\n",
      "call to mse_i_t lasted 2.4419095516204834\n",
      "4 0.488 16.32676601409912\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2380\u001b[0;31m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2381\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'dense_1_7/Relu' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m       \u001b[0mxla_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2383\u001b[0m       \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2385\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operation 'dense_1_7/Relu' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-752d3d1217ca>\u001b[0m in \u001b[0;36mtrain_new_model\u001b[0;34m(self, df, y)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_pool_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;31m#freeze all layers except last one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[0;34m(f, custom_objects, compile)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'optimizer_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Build train function (to get weight updates).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0moptimizer_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             optimizer_weight_names = [\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    508\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    510\u001b[0m                 updates = (self.updates +\n\u001b[1;32m    511\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             raise ValueError('An operation has `None` for gradient. '\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   2755\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m     \"\"\"\n\u001b[0;32m-> 2757\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    159\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 679\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    680\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaScope\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 679\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    680\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_ReluGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_ReluGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mrelu_grad\u001b[0;34m(gradients, features, name)\u001b[0m\n\u001b[1;32m  11152\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11153\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m> 11154\u001b[0;31m         \"ReluGrad\", gradients=gradients, features=features, name=name)\n\u001b[0m\u001b[1;32m  11155\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11156\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbase_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[0;32m--> 631\u001b[0;31m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m                                        param_name=input_name)\n\u001b[1;32m    633\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_Attr\u001b[0;34m(op_def, name)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   raise TypeError(\"Inconsistent OpDef for '%s', missing attr '%s'\" %\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dt = dtel(pool_size=25)\n",
    "\n",
    "r, preds, results = [], [], []\n",
    "correct_cnt = 0\n",
    "pool = []\n",
    "size = 500\n",
    "start = t.time()\n",
    "for i in range(120):\n",
    "    \n",
    "    theta = thetas_g[i//15]\n",
    "    X_train, y_train = buid_set(size, generator, theta)\n",
    "    if i != 0:\n",
    "        y_pred = dt.predict(X_train)\n",
    "        correct_cnt += sum(pd.DataFrame(y_pred)[0]==pd.DataFrame(y_train)[1])\n",
    "        preds.extend(y_pred)\n",
    "        results.append(sum(pd.DataFrame(y_pred)[0]==pd.DataFrame(y_train)[1])/size)\n",
    "        print(i, results[-1], t.time()-start)\n",
    "        start = t.time()\n",
    "    dt.train_new_model(X_train, y_train)\n",
    "    dt.update_pool(X_train, y_train)\n",
    "    pool.append(dt.pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1158067292325824233\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 17123479464010802845\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<keras.engine.sequential.Sequential at 0x7f6ade1c26d8>],\n",
       " [<keras.engine.sequential.Sequential at 0x7f6ade2ea828>,\n",
       "  <keras.engine.sequential.Sequential at 0x7f6ade3a7a90>],\n",
       " [<keras.engine.sequential.Sequential at 0x7f6ade148cf8>,\n",
       "  <keras.engine.sequential.Sequential at 0x7f6ade167eb8>,\n",
       "  <keras.engine.sequential.Sequential at 0x7f6ade24afd0>]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.09237836,  0.06928014, -0.1154742 , -0.05878377, -0.02426073],\n",
       "        [ 0.04625523,  0.05701505,  0.00864731, -0.00980854, -0.0073113 ],\n",
       "        [ 0.00019815, -0.0056593 ,  0.03725636, -0.03147896,  0.07602184]],\n",
       "       dtype=float32),\n",
       " array([ 0.02094292, -0.02009555,  0.02109781,  0.        ,  0.02090373],\n",
       "       dtype=float32),\n",
       " array([[-0.00924385,  0.12908757],\n",
       "        [ 0.02675428, -0.07053556],\n",
       "        [-0.05411606,  0.08099977],\n",
       "        [ 0.00080629,  0.05344507],\n",
       "        [-0.04452569,  0.01266712]], dtype=float32),\n",
       " array([-0.0196235,  0.0196235], dtype=float32)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool[0][0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.09237836,  0.06928014, -0.1154742 , -0.05878377, -0.02426073],\n",
       "        [ 0.04625523,  0.05701505,  0.00864731, -0.00980854, -0.0073113 ],\n",
       "        [ 0.00019815, -0.0056593 ,  0.03725636, -0.03147896,  0.07602184]],\n",
       "       dtype=float32),\n",
       " array([ 0.02094292, -0.02009555,  0.02109781,  0.        ,  0.02090373],\n",
       "       dtype=float32),\n",
       " array([[-0.00924385,  0.12908757],\n",
       "        [ 0.02675428, -0.07053556],\n",
       "        [-0.05411606,  0.08099977],\n",
       "        [ 0.00080629,  0.05344507],\n",
       "        [-0.04452569,  0.01266712]], dtype=float32),\n",
       " array([-0.0196235,  0.0196235], dtype=float32)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool[1][0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.09237836,  0.06928014, -0.1154742 , -0.05878377, -0.02426073],\n",
       "        [ 0.04625523,  0.05701505,  0.00864731, -0.00980854, -0.0073113 ],\n",
       "        [ 0.00019815, -0.0056593 ,  0.03725636, -0.03147896,  0.07602184]],\n",
       "       dtype=float32),\n",
       " array([ 0.02094292, -0.02009555,  0.02109781,  0.        ,  0.02090373],\n",
       "       dtype=float32),\n",
       " array([[-0.00924385,  0.12908757],\n",
       "        [ 0.02675428, -0.07053556],\n",
       "        [-0.05411606,  0.08099977],\n",
       "        [ 0.00080629,  0.05344507],\n",
       "        [-0.04452569,  0.01266712]], dtype=float32),\n",
       " array([-0.0196235,  0.0196235], dtype=float32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool[2][0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.09237836,  0.06928014, -0.1154742 , -0.05878377, -0.02426073],\n",
       "        [ 0.04625523,  0.05701505,  0.00864731, -0.00980854, -0.0073113 ],\n",
       "        [ 0.00019815, -0.0056593 ,  0.03725636, -0.03147896,  0.07602184]],\n",
       "       dtype=float32),\n",
       " array([ 0.02094292, -0.02009555,  0.02109781,  0.        ,  0.02090373],\n",
       "       dtype=float32),\n",
       " array([[-0.00924385,  0.12908757],\n",
       "        [ 0.02675428, -0.07053556],\n",
       "        [-0.05411606,  0.08099977],\n",
       "        [ 0.00080629,  0.05344507],\n",
       "        [-0.04452569,  0.01266712]], dtype=float32),\n",
       " array([-0.0196235,  0.0196235], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.pool[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.16694741,  0.12433422, -0.02733341,  0.01936467, -0.06072316],\n",
       "        [ 0.0830221 ,  0.05574498, -0.0251658 ,  0.04658831,  0.02046689],\n",
       "        [-0.06261133, -0.03913963,  0.07213517,  0.01632791,  0.0440337 ]],\n",
       "       dtype=float32),\n",
       " array([-0.02040634, -0.02020923,  0.02034671,  0.0101704 ,  0.02005466],\n",
       "       dtype=float32),\n",
       " array([[ 0.05844754,  0.00856236],\n",
       "        [ 0.0389089 , -0.02927567],\n",
       "        [-0.03226162,  0.03062434],\n",
       "        [ 0.0567523 ,  0.04109545],\n",
       "        [-0.15652652,  0.0207536 ]], dtype=float32),\n",
       " array([-0.01956593,  0.01956593], dtype=float32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.pool[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 146us/step - loss: 0.5855 - acc: 0.6771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5b13a3ba8>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "10000/10000 [==============================] - 1s 148us/step - loss: 0.6794 - acc: 0.6730 - val_loss: 0.6654 - val_acc: 0.6770\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 0s 18us/step - loss: 0.6482 - acc: 0.7328 - val_loss: 0.6330 - val_acc: 0.7660\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 0.6125 - acc: 0.7818 - val_loss: 0.5969 - val_acc: 0.7770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5dcae0128>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 128\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_4/kernel:0' shape=(3, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(3,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_5/kernel:0' shape=(3, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(2,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-3fb9055d0dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "model.weights[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-3473df2dcd5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 785       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 4         \n",
      "=================================================================\n",
      "Total params: 789\n",
      "Trainable params: 4\n",
      "Non-trainable params: 785\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1084 - acc: 0.9680 - val_loss: 0.1012 - val_acc: 0.9711\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1035 - acc: 0.9683 - val_loss: 0.0986 - val_acc: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e011e9470>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2\n",
    "batch_size = 128\n",
    "model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_3= d.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_1/kernel:0' shape=(784, 1) dtype=float32_ref>, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'dense_2/kernel:0' shape=(1, 2) dtype=float32_ref>, <tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32_ref>]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0981 - acc: 0.9718 - val_loss: 0.0926 - val_acc: 0.9733\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0944 - acc: 0.9728 - val_loss: 0.0917 - val_acc: 0.9726\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0917 - acc: 0.9736 - val_loss: 0.0884 - val_acc: 0.9747\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0899 - acc: 0.9739 - val_loss: 0.0888 - val_acc: 0.9752\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0888 - acc: 0.9743 - val_loss: 0.0881 - val_acc: 0.9756\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0875 - acc: 0.9745 - val_loss: 0.0869 - val_acc: 0.9754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42577be710>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.trainable = True\n",
    "print(model1.trainable_weights)\n",
    "\n",
    "epochs = 6\n",
    "batch_size = 128\n",
    "model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after[0] == after_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04466002],\n",
       "       [ 0.04611144],\n",
       "       [ 0.03130344],\n",
       "       [-0.03725884],\n",
       "       [ 0.01148854],\n",
       "       [ 0.00646817],\n",
       "       [-0.01815568],\n",
       "       [ 0.00515334],\n",
       "       [-0.02820878],\n",
       "       [ 0.01650673],\n",
       "       [-0.01893103],\n",
       "       [-0.0122538 ],\n",
       "       [-0.05547793],\n",
       "       [ 0.07850227],\n",
       "       [-0.03446824],\n",
       "       [ 0.08233008],\n",
       "       [-0.07949447],\n",
       "       [-0.02539072],\n",
       "       [ 0.05617425],\n",
       "       [ 0.04319464]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after[0][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = model1.trainable_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_2/kernel:0' shape=(1, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x7f1243b3d438>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_5/random_normal:0' shape=(784, 1) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.initial_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns the value of this variable, read in the current context.\n",
       "\n",
       "Can be different from value() if it's on another device, with control\n",
       "dependencies, etc.\n",
       "\n",
       "Returns:\n",
       "  A `Tensor` containing the value of the variable.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?l.read_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
